{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cbd11d0",
   "metadata": {},
   "source": [
    "# Taxi-v3 - Analyse d'agents\n",
    "\n",
    "## 1. Imports et création de l'environnement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d39941",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.13.9' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "state, info = env.reset(seed=42)\n",
    "print(\"State initial :\", state)\n",
    "print(\"Action space :\", env.action_space)\n",
    "print(\"Observation space :\", env.observation_space)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b02610",
   "metadata": {},
   "source": [
    "## 2. Visualisation de l'environnement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7588f8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae3e37e",
   "metadata": {},
   "source": [
    "## 3. Agent aléatoire (quelques épisodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fad309",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 5\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    print(f\"\\n=== Episode {ep+1} ===\")\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    print(f\"Steps: {steps}, Total reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992c6a65",
   "metadata": {},
   "source": [
    "## 4. Agent aléatoire instrumenté (baseline statistique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6749617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "num_episodes = 1000\n",
    "\n",
    "episode_rewards = []\n",
    "episode_steps = []\n",
    "episode_success = []\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    success = 0\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "        # Si l'épisode est terminé par \"terminé\" (et pas juste par max-steps),\n",
    "        # on considère que le taxi a réussi (dans Taxi-v3, fin normale = dropoff réussi).\n",
    "        if terminated:\n",
    "            success = 1\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    episode_steps.append(steps)\n",
    "    episode_success.append(success)\n",
    "\n",
    "random_stats = pd.DataFrame({\n",
    "    \"reward\": episode_rewards,\n",
    "    \"steps\": episode_steps,\n",
    "    \"success\": episode_success,\n",
    "})\n",
    "\n",
    "random_stats.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b45f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Episodes           :\", num_episodes)\n",
    "print(\"Reward moyen       :\", random_stats[\"reward\"].mean())\n",
    "print(\"Steps moyens       :\", random_stats[\"steps\"].mean())\n",
    "print(\"Taux de succès (%) :\", 100 * random_stats[\"success\"].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253ffe63",
   "metadata": {},
   "source": [
    "## 5. Agent Q-learning (mise en place)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc65ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamètres Q-learning\n",
    "alpha = 0.1          # learning rate\n",
    "gamma = 0.99         # discount factor\n",
    "epsilon = 1.0        # exploration initiale\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.999\n",
    "num_episodes_train = 20000\n",
    "\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "def choose_action(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()      # exploration\n",
    "    else:\n",
    "        return np.argmax(Q[state])           # exploitation\n",
    "\n",
    "training_rewards = []\n",
    "\n",
    "for ep in range(num_episodes_train):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = choose_action(state, epsilon)\n",
    "\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Q-learning update\n",
    "        best_next_action = np.argmax(Q[next_state])\n",
    "        td_target = reward + gamma * Q[next_state, best_next_action] * (0 if done else 1)\n",
    "        td_error = td_target - Q[state, action]\n",
    "        Q[state, action] += alpha * td_error\n",
    "\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "    # decay epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    training_rewards.append(total_reward)\n",
    "\n",
    "len(training_rewards), epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3de7c69",
   "metadata": {},
   "source": [
    "## 6. Évaluation de l’agent Q-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd47ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes_eval = 1000\n",
    "\n",
    "q_rewards = []\n",
    "q_steps = []\n",
    "q_success = []\n",
    "\n",
    "for ep in range(num_episodes_eval):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    success = 0\n",
    "\n",
    "    while not done:\n",
    "        # action = argmax Q[state] (politique déterministe, pas d'exploration)\n",
    "        action = np.argmax(Q[state])\n",
    "\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "        if terminated:\n",
    "            success = 1\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    q_rewards.append(total_reward)\n",
    "    q_steps.append(steps)\n",
    "    q_success.append(success)\n",
    "\n",
    "q_stats = pd.DataFrame({\n",
    "    \"reward\": q_rewards,\n",
    "    \"steps\": q_steps,\n",
    "    \"success\": q_success,\n",
    "})\n",
    "\n",
    "print(\"Episodes (Q-learning)   :\", num_episodes_eval)\n",
    "print(\"Reward moyen (Q)        :\", q_stats['reward'].mean())\n",
    "print(\"Steps moyens (Q)        :\", q_stats['steps'].mean())\n",
    "print(\"Taux de succès Q (%)    :\", 100 * q_stats['success'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b23285",
   "metadata": {},
   "source": [
    "## 7. Visualisation de l'apprentissage (Q-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9cf582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "window = 100\n",
    "smoothed_rewards = np.convolve(training_rewards, np.ones(window)/window, mode=\"valid\")\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(smoothed_rewards)\n",
    "plt.xlabel(f\"Épisode (moyenne glissante sur {window})\")\n",
    "plt.ylabel(\"Reward moyen\")\n",
    "plt.title(\"Évolution du reward pendant l'entraînement (Q-learning)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c24a089",
   "metadata": {},
   "source": [
    "## 8. Où l'agent reste inefficace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51211a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 des épisodes Q-learning les plus longs\n",
    "q_stats.sort_values(\"steps\", ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3941201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rejouer un épisode Q-learning long pour inspecter la trajectoire\n",
    "state, info = env.reset()\n",
    "done = False\n",
    "steps = 0\n",
    "\n",
    "print(\"Épisode Q-learning (inspection)\")\n",
    "env.render()\n",
    "\n",
    "while not done and steps < 50:  # limite d'affichage\n",
    "    action = np.argmax(Q[state])\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    steps += 1\n",
    "\n",
    "    print(f\"Step {steps} | Action: {action} | Reward: {reward}\")\n",
    "    env.render()\n",
    "\n",
    "    state = next_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f994a27",
   "metadata": {},
   "source": [
    "Même avec 100 % de succès, certains épisodes restent relativement longs ou passent par des trajectoires peu efficaces.\n",
    "Cela montre que la performance brute (succès / échec) ne suffit pas à juger la qualité d'un agent : \n",
    "il faut aussi regarder la structure des décisions, les détours inutiles et la stabilité du comportement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032e07b8",
   "metadata": {},
   "source": [
    "## 9. Comment généraliser à des agents réels\n",
    "\n",
    "Dans ce projet, l’agent Taxi-v3 apprend à optimiser une politique dans un environnement simple et parfaitement connu.\n",
    "La même logique d'instrumentation (logs d'épisodes, définition de KPI, comparaison de politiques) peut s'appliquer à des agents déployés en entreprise :\n",
    "\n",
    "- agent de support client : mesurer taux de résolution, satisfaction, temps moyen de prise en charge ;\n",
    "- agent logistique : mesurer respect des contraintes, coût total, délais de livraison ;\n",
    "- agent de recommandation : mesurer clics, conversion, rétention.\n",
    "\n",
    "Ce mini-environnement sert donc de sandbox pour expérimenter des pratiques d’“Agent Ops” (observabilité, métriques, amélioration itérative) avant de les appliquer à des systèmes d’agents plus complexes, comme ceux présentés dans le 5-Day AI Agents Intensive (multi-agents, outils, mémoire, A/B testing, etc.).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
